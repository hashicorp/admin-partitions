This folder is used to deploy 3 eks clusters which will be used to deploy Consul onto.
- Consul Server cluster
- Consul Client 1 cluster
- Consul Client 2 cluster

Pre-reqs: The follow resources will be used as inputs to creating the EKS cluster.
  - You already have a VPC
  - You already have 3 public subnets, which are routeble to eachother (they should already be by default)
  - You have added labels as seen below to the each subnets. This allows Elastic LBs to be created by the EKS cluster.
     Key – kubernetes.io/role/elb
     Value – 1
  - You should create a Security Group that has open access for all-inbound and outboudn access for any source and destination.
    This is for the purposes only, not production.  

This has been depoyed using a mac and should also work on linux-like systems.
Copy or clone the whole repo to your local system.

DEPLOY 3 EKS CLUSTERS

 - In the terraform directory, edit the variable.tf file to reflect your desired vpc, region, subnets, security group.


DEPLOY CONSUL SERVERS TO EKS SERVER CLUSTER 

- Set context for the EKS Server cluster:

  aws eks --region <your_aws_region> update-kubeconfig --name <eks_server_cluster_name>
  
- Upload Consul Ent license as a kube secret to your EKS cluster. Trial license can be generated here:
  https://www.hashicorp.com/products/consul/trial
  
  Example:

  kubectl create secret generic license --from-literal=key=02MV4UU43BK5HGYYTOJZWFQMTMNNEWU33JJYZFU3KNPJHGQTKEKF2FS3KRGFHGSMBSLJVGQ2SMKRATCTSHJF2E23KFGNHHUZZULJWVSMKPK5FGYSLJO5UVSM2WPJSEOOLULJMEUZTBK5IWST3JJF4FS2THPFHUITJRJZ4TC22PKRTXUTCUIF4E2MSVORHUORJRJZ4TAMS2K5MTGTTKJEYU6RCRGFHUIY3JJRBUU4DCNZHDAWKXPBZVSWCSOBRDENLGMFLVC2KPNFEXCSLJO5UWCWCOPJSFOVTGMRDWY5C2KNETMSLKJF3U22SFORGVISLUJVCGIVKNNJATMTLKKE3E2VCVOVHEIY3ZJVVES6SOIRTXSV3JJFZUS3SOGBMVQSRQLAZVE4DCK5KWST3JJF4U2RCJPBGFIRLZJRKECM2WIREXOT3KJEYE62SFGFLWSSLTJFWVMNDDI5WHSWKYKJYGEMRVMZSEO3DULJJUSNSJNJEXOTLKJF2E2RCFORGUIWSVJVVECNSNNJITMTKUKZQUS2LXNFSEOVTZMJLWY5KZLBJHAYRSGVTGIR3MORNFGSJWJFVES52NNJEXITKEIV2E2RDEKVGWUQJWJVVFCNSNKRLGCSLJO5UWGSCKOZNEQVTKMRBUSNSJNVHHMYTOJYYWEQ2JONEW2WTTLFLWI6SJNJYDOSLNGF3FUSCWONNFQTLJJ5WHG2K2GJ4HMWLNIZZUYWC2OBRTE3DJMFLXQ4DEJBVXIY3NHEYWIR3MOVNHSML2LEZEM422KNEXGSLNMR3GI3KWPFRG2RTVLEZFK5DDI44XGYKXJY2US3BRHFTFCPJ5FZTXETSDOFUU6SDMJVDDQT2NPJYXCR2UGI2DAMBPGY2WE3ZTMIYSWSKBKR2WURKXJ5DUGWRYGNUU2MLBNRBDIQ2KON2VOZTMGJCGQODQIFLVMOLJGE2WC4DZI5CFCNSCNVCUYNSZIRNDCZCBNNRWCNDEKYYDGMZYKB3W2VTMMF3EUUBUOBFHQSKJHFCDMVKGJRKWCVSQNJVVOSTUMNCDM4DBNQ3G6T3GI5XEWMT2KBFUUUTNI5EFMM3FLJ3XCRTFFNXTO2ZPOMVUCVCONBIFUZ2TF5FVMWLHF5FSW3CHKB3UYN3KIJ4ESN2HJ5QWWNSVMFUWCSDPMVVTAUSUN43TERCRHU6Q

- Run update to get the latest consul chart:

  helm repo update 

- Install consul server. Note: For this example, we will deploy with tls and ACL’s enabled in our helm chart.

  helm install consul hashicorp/consul -f helm-server.yaml --wait --debug
  
- Confirm Consul pods have successfully deployed:
  eks-3-clusters-v3 % kubectl get pods    
  NAME                                                              READY   STATUS    RESTARTS   AGE
  consul-consul-cnv8z                                               1/1     Running   0          103s
  consul-consul-connect-injector-webhook-deployment-5d8ff7f44q529   1/1     Running   0          102s
  consul-consul-connect-injector-webhook-deployment-5d8ff7f4mkmsx   1/1     Running   0          102s
  consul-consul-controller-7cb5ddfbbc-mp5rh                         1/1     Running   0          102s
  consul-consul-hj4rf                                               1/1     Running   0          103s
  consul-consul-jrwwf                                               1/1     Running   0          103s
  consul-consul-mesh-gateway-7f9bf7669b-cznmf                       2/2     Running   0          102s
  consul-consul-mesh-gateway-7f9bf7669b-qlckr                       2/2     Running   0          102s
  consul-consul-server-0                                            1/1     Running   0          102s
  consul-consul-webhook-cert-manager-6bd65f67f7-2jqh6               1/1     Running   0          102s



- Once deployed, check that all services are up:

  kubectl get services
  NAME                                 TYPE           CLUSTER-IP       EXTERNAL-IP                                                                PORT(S)                                                                   AGE
  consul-consul-connect-injector-svc   ClusterIP      172.20.114.128   <none>                                                                     443/TCP                                                                   18s
  consul-consul-controller-webhook     ClusterIP      172.20.43.214    <none>                                                                     443/TCP                                                                   18s
  consul-consul-dns                    ClusterIP      172.20.186.241   <none>                                                                     53/TCP,53/UDP                                                             18s
  consul-consul-mesh-gateway           LoadBalancer   172.20.212.5     a00a823716a7d4597b68b65072f970b6-1765211840.eu-north-1.elb.amazonaws.com   443:31218/TCP                                                             18s
  consul-consul-partition-service      LoadBalancer   172.20.138.45    a45c0d48015ed418fbebb1154604f20c-163347652.eu-north-1.elb.amazonaws.com    8501:31819/TCP,8301:31259/TCP,8300:32170/TCP                              18s
  consul-consul-server                 ClusterIP      None             <none>                                                                     8501/TCP,8301/TCP,8301/UDP,8302/TCP,8302/UDP,8300/TCP,8600/TCP,8600/UDP   18s
  consul-consul-ui                     LoadBalancer   172.20.86.24     a147f1516f6874a91851f398f7034527-38406048.eu-north-1.elb.amazonaws.com     443:31865/TCP                                                             18s
  kubernetes                           ClusterIP      172.20.0.1       <none>                                                                     443/TCP                                                                   50m

- Make a note of the External-IP of the consul-consul-partition-service. We will use it when we deploy the next Consul Client. 


- View Consul UI in a browser using the External-IP address of the consul-consul-ui service above. 

  https://a147f1516f6874a91851f398f7034527-38406048.eu-north-1.elb.amazonaws.com:443

  Notice only default partition exists.

DEPLOY CONSUL CLIENT 1 ONTO EKS CLIENT CLUSTER 1

- Set context for the EKS Client 1 cluster:

  aws eks --region <your_aws_region> update-kubeconfig --name <eks_client_1_cluster_name>
  
- Upload Consul Ent license as a kube secret to your EKS cluster. Same license can be reused from earlier step.
  
  Example:

  kubectl create secret generic license --from-literal=key=02MV4UU43BK5HGYYTOJZWFQMTMNNEWU33JJYZFU3KNPJHGQTKEKF2FS3KRGFHGSMBSLJVGQ2SMKRATCTSHJF2E23KFGNHHUZZULJWVSMKPK5FGYSLJO5UVSM2WPJSEOOLULJMEUZTBK5IWST3JJF4FS2THPFHUITJRJZ4TC22PKRTXUTCUIF4E2MSVORHUORJRJZ4TAMS2K5MTGTTKJEYU6RCRGFHUIY3JJRBUU4DCNZHDAWKXPBZVSWCSOBRDENLGMFLVC2KPNFEXCSLJO5UWCWCOPJSFOVTGMRDWY5C2KNETMSLKJF3U22SFORGVISLUJVCGIVKNNJATMTLKKE3E2VCVOVHEIY3ZJVVES6SOIRTXSV3JJFZUS3SOGBMVQSRQLAZVE4DCK5KWST3JJF4U2RCJPBGFIRLZJRKECM2WIREXOT3KJEYE62SFGFLWSSLTJFWVMNDDI5WHSWKYKJYGEMRVMZSEO3DULJJUSNSJNJEXOTLKJF2E2RCFORGUIWSVJVVECNSNNJITMTKUKZQUS2LXNFSEOVTZMJLWY5KZLBJHAYRSGVTGIR3MORNFGSJWJFVES52NNJEXITKEIV2E2RDEKVGWUQJWJVVFCNSNKRLGCSLJO5UWGSCKOZNEQVTKMRBUSNSJNVHHMYTOJYYWEQ2JONEW2WTTLFLWI6SJNJYDOSLNGF3FUSCWONNFQTLJJ5WHG2K2GJ4HMWLNIZZUYWC2OBRTE3DJMFLXQ4DEJBVXIY3NHEYWIR3MOVNHSML2LEZEM422KNEXGSLNMR3GI3KWPFRG2RTVLEZFK5DDI44XGYKXJY2US3BRHFTFCPJ5FZTXETSDOFUU6SDMJVDDQT2NPJYXCR2UGI2DAMBPGY2WE3ZTMIYSWSKBKR2WURKXJ5DUGWRYGNUU2MLBNRBDIQ2KON2VOZTMGJCGQODQIFLVMOLJGE2WC4DZI5CFCNSCNVCUYNSZIRNDCZCBNNRWCNDEKYYDGMZYKB3W2VTMMF3EUUBUOBFHQSKJHFCDMVKGJRKWCVSQNJVVOSTUMNCDM4DBNQ3G6T3GI5XEWMT2KBFUUUTNI5EFMM3FLJ3XCRTFFNXTO2ZPOMVUCVCONBIFUZ2TF5FVMWLHF5FSW3CHKB3UYN3KIJ4ESN2HJ5QWWNSVMFUWCSDPMVVTAUSUN43TERCRHU6Q

- Copy secrets (TLS server certificate and key) from consul server EKS cluster to client 1 EKS cluster. This is needed b/c TLS is enabled.

  kubectl get secret consul-consul-ca-cert --context <Consul-Server-EKS-cluster-name> -o yaml | kubectl apply --context <Consul-Client-1-EKS-cluster-name> -f -

  kubectl get secret consul-consul-ca-key --context <Consul-Server-EKS-cluster-name> -o yaml | kubectl apply --context <Consul-Client-1-EKS-cluster-name> -f -

- Copy over the partitions token from server EKS cluster onto the client K8s cluster. This is needed since ACLs are enabled:

  kubectl get secret consul-consul-partitions-acl-token --context <Consul-Server-EKS-cluster-name> -o json | kubectl apply --context <Client-1-EKS-cluster-name> -f -

- Edit Consul client1 helm file with the field below: helm-client-team1.yaml
  
   a) Edit both the "hosts" and "join" fields to reflect the External-IP (or DNS name) of the consul-consul-partition-service on the Server EKS cluster. 
      You should have noted the External-IP was mentioned in the previous steps above.
      
   b) Edit the "k8sAuthMethodHost" field to reflect the EKS API server's IP/DNS name of the Client 1 EKS Cluster. You can retreive the DNS name by running: 
  
      kubectl config view -o jsonpath="{.clusters[?(@.name=='<Consul-Client-1-EKS-cluster-name>')].cluster.server}"
  
  Example inputs for the helm-client-team1.yaml file:
  
  externalServers:
    enabled: true
    hosts: [ "a45c0d48015ed418fbebb1154604f20c-163347652.eu-north-1.elb.amazonaws.com" ]
    tlsServerName: server.dc1.consul
    k8sAuthMethodHost: "E9122C19C8B28D0DFBC6E0C7DDF16B4D.yl4.eu-north-1.eks.amazonaws.com"
  ...
  client:
    enabled: true
    exposeGossipPorts: true
    join: [ "a45c0d48015ed418fbebb1154604f20c-163347652.eu-north-1.elb.amazonaws.com" ]


- Install consul client 1 and join server cluster:
    
  helm install consul hashicorp/consul -f helm-client-team1.yaml
  
- Confirm Consul clients deployed successfully. You will see only consul client, and no consul server.
 
  kubectl get pods    
  NAME                                                              READY   STATUS    RESTARTS   AGE
  consul-consul-8dnz2                                               1/1     Running   0          104s
  consul-consul-connect-injector-webhook-deployment-768dccbdttfvz   1/1     Running   0          103s
  consul-consul-connect-injector-webhook-deployment-768dccbdvh9j5   1/1     Running   0          103s
  consul-consul-controller-58994c5b86-md2wl                         1/1     Running   0          103s
  consul-consul-dbpgm                                               1/1     Running   0          104s
  consul-consul-f854k                                               1/1     Running   0          104s
  consul-consul-mesh-gateway-7b9b598c8d-8z64k                       2/2     Running   0          103s
  consul-consul-mesh-gateway-7b9b598c8d-zmfx9                       2/2     Running   0          103s
  consul-consul-webhook-cert-manager-6bd65f67f7-rldbb               1/1     Running   0          103s

- View in Consul UI on browser. You will need the login to the UI with a higher priviledge ACL token. For demo/test purposes, you use the bootstrap token that
  that was generated on the Consul server EKS cluster. To retrieve the token, run: 
  
   kubectl get secret consul-consul-bootstrap-acl-token --context <Consul-Server-EKS-cluster-name> EKS-o jsonpath="{.data.token}" | base64 --decode
  
  Once logged onto browser, you should bee a new team1 admin partition was created in the Admin Partition box. 
  This partition name was set in the helm values file: helm-client-team1.yaml


DEPLOY CONSUL CLIENT 2 ONTO EKS CLIENT CLUSTER 2 (this is mostly a repeat of client cluster 1)

- Set context for the EKS Client 2 cluster:

  aws eks --region <your_aws_region> update-kubeconfig --name <eks_client_2_cluster_name>
  
- Upload Consul Ent license as a kube secret to your EKS cluster. Same license can be reused from earlier step.
  
  Example:

  kubectl create secret generic license --from-literal=key=02MV4UU43BK5HGYYTOJZWFQMTMNNEWU33JJYZFU3KNPJHGQTKEKF2FS3KRGFHGSMBSLJVGQ2SMKRATCTSHJF2E23KFGNHHUZZULJWVSMKPK5FGYSLJO5UVSM2WPJSEOOLULJMEUZTBK5IWST3JJF4FS2THPFHUITJRJZ4TC22PKRTXUTCUIF4E2MSVORHUORJRJZ4TAMS2K5MTGTTKJEYU6RCRGFHUIY3JJRBUU4DCNZHDAWKXPBZVSWCSOBRDENLGMFLVC2KPNFEXCSLJO5UWCWCOPJSFOVTGMRDWY5C2KNETMSLKJF3U22SFORGVISLUJVCGIVKNNJATMTLKKE3E2VCVOVHEIY3ZJVVES6SOIRTXSV3JJFZUS3SOGBMVQSRQLAZVE4DCK5KWST3JJF4U2RCJPBGFIRLZJRKECM2WIREXOT3KJEYE62SFGFLWSSLTJFWVMNDDI5WHSWKYKJYGEMRVMZSEO3DULJJUSNSJNJEXOTLKJF2E2RCFORGUIWSVJVVECNSNNJITMTKUKZQUS2LXNFSEOVTZMJLWY5KZLBJHAYRSGVTGIR3MORNFGSJWJFVES52NNJEXITKEIV2E2RDEKVGWUQJWJVVFCNSNKRLGCSLJO5UWGSCKOZNEQVTKMRBUSNSJNVHHMYTOJYYWEQ2JONEW2WTTLFLWI6SJNJYDOSLNGF3FUSCWONNFQTLJJ5WHG2K2GJ4HMWLNIZZUYWC2OBRTE3DJMFLXQ4DEJBVXIY3NHEYWIR3MOVNHSML2LEZEM422KNEXGSLNMR3GI3KWPFRG2RTVLEZFK5DDI44XGYKXJY2US3BRHFTFCPJ5FZTXETSDOFUU6SDMJVDDQT2NPJYXCR2UGI2DAMBPGY2WE3ZTMIYSWSKBKR2WURKXJ5DUGWRYGNUU2MLBNRBDIQ2KON2VOZTMGJCGQODQIFLVMOLJGE2WC4DZI5CFCNSCNVCUYNSZIRNDCZCBNNRWCNDEKYYDGMZYKB3W2VTMMF3EUUBUOBFHQSKJHFCDMVKGJRKWCVSQNJVVOSTUMNCDM4DBNQ3G6T3GI5XEWMT2KBFUUUTNI5EFMM3FLJ3XCRTFFNXTO2ZPOMVUCVCONBIFUZ2TF5FVMWLHF5FSW3CHKB3UYN3KIJ4ESN2HJ5QWWNSVMFUWCSDPMVVTAUSUN43TERCRHU6Q

- Copy secrets (TLS server certificate and key) from consul server EKS cluster to client 2 EKS cluster. This is needed b/c TLS is enabled.

  kubectl get secret consul-consul-ca-cert --context <Consul-Server-EKS-cluster-name> -o yaml | kubectl apply --context <Consul-Client-2-EKS-cluster-name> -f -

  kubectl get secret consul-consul-ca-key --context <Consul-Server-EKS-cluster-name> -o yaml | kubectl apply --context <Consul-Client-2-EKS-cluster-name> -f -

- Copy over the partitions token from server EKS cluster onto the client K8s cluster. This is needed since ACLs are enabled:

  kubectl get secret consul-consul-partitions-acl-token --context <Consul-Server-EKS-cluster-name> -o json | kubectl apply --context <Consul-Client-2-EKS-cluster-name> -f -
  
- Edit Consul client2 helm file with the field below: helm-client-team2.yaml
  
   a) Edit both the "hosts" and "join" fields to reflect the External-IP (or DNS name) of the consul-consul-partition-service on the Server EKS cluster.. 
      You should have noted the External-IP was mentioned in the previous steps above.
      
   b) Edit the "k8sAuthMethodHost" field to reflect the EKS API server's IP/DNS name of the Client 2 EKS Cluster. You can retreive the DNS name by running: 
  
      kubectl config view -o jsonpath="{.clusters[?(@.name=='<Consul-Client-2-EKS-cluster-name>')].cluster.server}"
  
  Example inputs for the helm-client-team2.yaml file:
  
  externalServers:
    enabled: true
    hosts: [ "a45c0d48015ed418fbebb1154604f20c-163347652.eu-north-1.elb.amazonaws.com" ]
    tlsServerName: server.dc1.consul
    k8sAuthMethodHost: "33AD1880FB671DB0E9DD6D56BED87E27.sk1.eu-north-1.eks.amazonaws.com"
  ...
  client:
    enabled: true
    exposeGossipPorts: true
    join: [ "a45c0d48015ed418fbebb1154604f20c-163347652.eu-north-1.elb.amazonaws.com" ]

- Install consul client 2 and join consul server cluster:
    
  helm install consul hashicorp/consul -f helm-client-team2.yaml
  
- Once successfully deployed and joined to server cluster, view in Consul browser. 
  You should bee a new team2 admin partition was created in the Admin Partition box.   

DEPLOY SERVICES

- You can now deploy an example counting service to a partition. Set the context for the Consul Client 1, which was deployed into partition "team1"

  aws eks --region <your_aws_region> update-kubeconfig --name <eks_client_1_cluster_name>

- Navigate to countingapp directory and deploy counting app:
  
  kubectl apply -f counting.yaml
  kubectl apply -f dashboard.yaml 
  
- View services in Consul UI, under team1 parttion. Two services should show up.  
  
- View counting app in browser:

  Set port forwarding for counting app service:
  
  kubectl port-forward service/dashboard 8888:9002
  
  Go to browser: http://127.0.0.1:8888/
  Browser should come up but the display will show -1 which means the dashboard is not communicating with the backend.
  This is b/c by default, intentions are deny all by default. Enable intention in the Consul UI and revisit the dashboard on browser.
  Counting should increment every ~10 seconds.
  
  
DEPLOY SERVICES ACROSS PARTITIONS 

- Set the context for the Consul Client 1, which was deployed into partition "team1"

  aws eks --region <your_aws_region> update-kubeconfig --name <eks_client_1_cluster_name>

- Navigate to fakeapp directory and deploy ONLY the frontend service. This is deploy into partition team1.

  kubectl apply -f frontend.yaml

- Enable Service Exports for frontend (partition team1)

  kubectl apply -f export-front.yaml 

- Set the context for the Consul Client 2, which was deployed into partition "team2"

  aws eks --region <your_aws_region> update-kubeconfig --name <eks_client_2_cluster_name>

- Deploy ONLY the backend service. This is deploy into partition team2.  

  kubectl apply -f backend.yaml

- Enable Service Exports for backend (partition team2)

  kubectl apply -f export-back.yaml 


